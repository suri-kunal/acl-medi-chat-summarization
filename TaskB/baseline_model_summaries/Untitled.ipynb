{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f95386c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 13:59:38.644548: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-18 13:59:39.540155: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import sys\n",
    "import pdb\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from sectiontagger import SectionTagger\n",
    "section_tagger = SectionTagger()\n",
    "\n",
    "\n",
    "SECTION_DIVISIONS = ['subjective', 'objective_exam', 'objective_results', 'assessment_and_plan']\n",
    "\n",
    "TASKA_RANGE = [0,100]\n",
    "TASKA_PREFIX = 'taskA'\n",
    "\n",
    "TASKB_RANGE = [88,127]\n",
    "TASKB_PREFIX = 'D2N'\n",
    "\n",
    "def add_section_divisions(row, model_name, dialogue_column ):\n",
    "    row['src_len'] = len(row[ dialogue_column ].split())\n",
    "#     pdb.set_trace()\n",
    "    for evaltype in ['note', model_name]:\n",
    "        text = row[evaltype]\n",
    "        text_with_endlines = text.replace( '__lf1__', '\\n' )\n",
    "        detected_divisions = section_tagger.divide_note_by_metasections(text_with_endlines)\n",
    "        for detected_division in detected_divisions:\n",
    "            label, _, _, start, _, end = detected_division\n",
    "            row[ '%s_%s' % (evaltype, label)] = text_with_endlines[start:end].replace('\\n', '__lf1__')\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63547a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ROUGE, BERTScore, BLEURT from HuggingFace\n",
      "INFO:tensorflow:Reading checkpoint /root/.cache/huggingface/metrics/bleurt/BLEURT-20/downloads/extracted/cd1c38739d180ae53192201859a058307621534b704c20700072eca17d748c58/BLEURT-20.\n",
      "INFO:tensorflow:Config file found, reading.\n",
      "INFO:tensorflow:Will load checkpoint BLEURT-20\n",
      "INFO:tensorflow:Loads full paths and checks that files exists.\n",
      "INFO:tensorflow:... name:BLEURT-20\n",
      "INFO:tensorflow:... bert_config_file:bert_config.json\n",
      "INFO:tensorflow:... max_seq_length:512\n",
      "INFO:tensorflow:... vocab_file:None\n",
      "INFO:tensorflow:... do_lower_case:None\n",
      "INFO:tensorflow:... sp_model:sent_piece\n",
      "INFO:tensorflow:... dynamic_seq_length:True\n",
      "INFO:tensorflow:Creating BLEURT scorer.\n",
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n",
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n",
      "INFO:tensorflow:Will load model: /root/.cache/huggingface/metrics/bleurt/BLEURT-20/downloads/extracted/cd1c38739d180ae53192201859a058307621534b704c20700072eca17d748c58/BLEURT-20/sent_piece.model.\n",
      "INFO:tensorflow:SentencePiece tokenizer created.\n",
      "INFO:tensorflow:Creating Eager Mode predictor.\n",
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 13:59:50.709494: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-18 13:59:50.711615: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    }
   ],
   "source": [
    "print('Loading ROUGE, BERTScore, BLEURT from HuggingFace')\n",
    "scorers = {\n",
    "    'rouge': (\n",
    "        evaluate.load('rouge'),\n",
    "        {'use_aggregator': False},\n",
    "        ['rouge1', 'rouge2', 'rougeL', 'rougeLsum'],\n",
    "        ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
    "    ),\n",
    "    'bert_scorer': (\n",
    "        evaluate.load('bertscore'),\n",
    "        {'model_type': 'microsoft/deberta-xlarge-mnli','batch_size':8},\n",
    "        ['precision', 'recall', 'f1'],\n",
    "        ['bertscore_precision', 'bertscore_recall', 'bertscore_f1']\n",
    "    ),\n",
    "    'bleurt': (\n",
    "        evaluate.load('bleurt', config_name='BLEURT-20'),\n",
    "        {},\n",
    "        ['scores'],\n",
    "        ['bleurt']\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b573222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_aggregate(obj, indices):\n",
    "\n",
    "    agg_obj = {}\n",
    "    for k, v in obj.items():\n",
    "        agg_obj[k] = [float(np.mean([v[i] for i in indices]))]\n",
    "    return agg_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5448bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scorers: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [03:24<00:00, 68.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to dialogled-large_1_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scorers: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [03:09<00:00, 63.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to dialogled-base_1_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scorers: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [02:59<00:00, 59.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to dialogled-large_2_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scorers: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [02:56<00:00, 58.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to dialogled-base_2_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scorers: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [03:06<00:00, 62.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to dialogled-large_0_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scorers: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [03:06<00:00, 62.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to dialogled-base_0_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scorers: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [03:00<00:00, 60.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to dialogled-large_3_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scorers: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [02:57<00:00, 59.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to dialogled-base_3_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scorers: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [02:58<00:00, 59.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to dialogled-large_4_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scorers: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [02:55<00:00, 58.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to dialogled-base_4_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for csv_file in Path.cwd().glob(\"*.csv\"):\n",
    "    model_architectures = [\"dialogled-large_summary\",\"dialogled-base_summary\"]    \n",
    "    split = int(csv_file.stem.split(\"_\")[1])\n",
    "    for model_name in model_architectures:        \n",
    "        model_arch = model_name.split(\"_\")[0]\n",
    "        full_df = pd.read_csv(csv_file)[[\"dataset\",\"note\",model_name,\"dialogue\"]]\n",
    "        references = full_df[\"note\"].tolist()\n",
    "        predictions = full_df[model_name].tolist()\n",
    "        num_test = len(full_df)\n",
    "        \n",
    "        full_df = full_df.apply( lambda row: add_section_divisions( row, model_name, \"dialogue\" ), axis=1)\n",
    "\n",
    "        # ===========CHECKS TO MAKE SURE THERE ARE SECTIONS ==========\n",
    "        total_detected_sections = sum([\n",
    "            full_df[f'{model_name}_{division}'].notna().sum() for division in SECTION_DIVISIONS\n",
    "        ])\n",
    "        if total_detected_sections == 0:\n",
    "            print('We detected 0 sections! - you can use override_section_check flag to run while ignoring this.')\n",
    "            sys.exit(1)\n",
    "\n",
    "        # Fill in missing section divisions as empty string\n",
    "        full_df.fillna('#####EMPTY#####', inplace=True)\n",
    "\n",
    "        ######## ADD INSTANCES FOR SECTION DIVISION ########\n",
    "        for division in SECTION_DIVISIONS:\n",
    "            null_default = [''] * num_test\n",
    "            references.extend(full_df.get(f'note_{division}', null_default))\n",
    "            predictions.extend(full_df.get(f'{model_name}_{division}', null_default))\n",
    "\n",
    "        # sanity check, we should now have 5 x the original set (one for full note, 4 for the divisions)\n",
    "        rn = len(references)\n",
    "        pn = len(predictions)\n",
    "        en = len(full_df) * 5\n",
    "        assert rn == pn == en, f'The number of references ({rn}) and predictions ({pn}) does not match expected ({en})'\n",
    "        \n",
    "        \n",
    "        all_scores = {}\n",
    "        for name, (scorer, kwargs, keys, save_keys) in tqdm(scorers.items(),desc=\"scorers\"):\n",
    "            scores = scorer.compute(references=references, predictions=predictions, **kwargs)\n",
    "            for score_key, save_key in zip(keys, save_keys):\n",
    "                all_scores[save_key] = scores[score_key]\n",
    "\n",
    "        cohorts = [\n",
    "            ('all', list(range(num_test))),\n",
    "        ]\n",
    "\n",
    "        subsets = full_df['dataset'].unique().tolist()\n",
    "        for subset in subsets:\n",
    "            # Don't include anything after num_test (section-level)\n",
    "            indices = full_df[full_df['dataset'] == subset].index.tolist()\n",
    "            cohorts.append((f'dataset-{subset}', indices))\n",
    "            \n",
    "        for ind, division in enumerate(SECTION_DIVISIONS):\n",
    "            start = (ind + 1) * num_test\n",
    "            end = (ind + 2) * num_test\n",
    "            cohorts.append((f'division-{division}', list(range(start, end))))\n",
    "\n",
    "\n",
    "        # ######## CALCULATE PER-LENGTH SCORES (bigger than --note_length_cutoff=512 vs not) ########\n",
    "        df_shortsrc = full_df[full_df['src_len'] <= 512]\n",
    "        if len(df_shortsrc) > 0:\n",
    "            indices = df_shortsrc.index.tolist()\n",
    "            cohorts.append(('shorter-src', indices))\n",
    "\n",
    "        df_longsrc = full_df[full_df['src_len'] > 512]\n",
    "        if len(df_longsrc) > 0:\n",
    "            indices = df_longsrc.index.tolist()\n",
    "            cohorts.append(('longer-src', indices))\n",
    "\n",
    "\n",
    "        outputs = {k: filter_and_aggregate(all_scores, idxs) for (k, idxs) in cohorts}\n",
    "\n",
    "        # ###### OUTPUT TO JSON FILE ########\n",
    "        fn_out = f'{model_arch}_{split}_results.json'\n",
    "        print(f'Saving results to {fn_out}')\n",
    "        with open(fn_out, 'w') as fd:\n",
    "            json.dump(outputs, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b31df426",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_arch_list = [\"dialogled-base\",\"dialogled-large\"]\n",
    "json_file_list = Path.cwd().glob(\"*.json\")\n",
    "df_dict = dict()\n",
    "for json_file in json_file_list:\n",
    "    with open(json_file,\"r\") as f:\n",
    "        json_data = json.load(f)\n",
    "    for key in json_data.keys():\n",
    "        model_with_split = json_file.stem.split(\"_\",maxsplit=-1)[0]\n",
    "        df = pd.DataFrame.from_dict(json_data[key])\n",
    "        df.rename(mapper={0:model_with_split},inplace=True)\n",
    "        if key not in df_dict:\n",
    "            df_dict[key] = df\n",
    "        else:\n",
    "            df_dict[key] = pd.concat([df_dict[key],df],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9e47c3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all\n",
      "dataset-virtassist\n",
      "dataset-virtscribe\n",
      "dataset-aci\n",
      "division-subjective\n",
      "division-objective_exam\n",
      "division-objective_results\n",
      "division-assessment_and_plan\n",
      "longer-src\n"
     ]
    }
   ],
   "source": [
    "df_grouped_dict = dict()\n",
    "for cohort in df_dict:\n",
    "    print(cohort)\n",
    "    df = df_dict[cohort].reset_index().groupby(\"index\").mean().reset_index()\n",
    "    df = df.rename(mapper={\"index\":\"model_architecure\"},axis=1)\n",
    "    df_grouped_dict[cohort] = df.to_dict()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7687392e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"all_sections_json.json\",\"w\") as f:\n",
    "    json.dump(df_grouped_dict,f,indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91d66fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c965fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94565e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a3c610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fde8a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129e0c18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
